{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Taxonomy Induction\n",
    "\n",
    "<br>\n",
    "In this exercise, we will perform the various steps commonly employed for unsupervised taxonomy induction from text corpora. Taxonomy induction from text typically consists of three main steps:\n",
    "\n",
    "<ol>\n",
    "  <li><b>Relations Extraction:</b> In this step, we use lexico-syntactic patterns to extract <b>IsA</b> relations from text. An example of IsA relation is (<i>apple, fruit</i>), which implies that <i>apple</i> is a type of fruit.</li>\n",
    "  <br>\n",
    "  <li><b>Initial Graph Construction:</b> In this step, we aggregate the extracted IsA relations to construct a potentially-noisy initial hypernym graph.</li>\n",
    "  <br>\n",
    "  <li><b>Graph Pruning:</b> In the final step, we perform some pruning or optimization steps to induce the a final clean taxonomy.</li>\n",
    "</ol>  \n",
    "\n",
    "<br>\n",
    "We will describe these steps in detail in the rest of this exercise. \n",
    "\n",
    "\n",
    "## Question 1 - Relations Extraction\n",
    "\n",
    "\n",
    "In this part of the exercise, we will run a small-scale extraction of IsA relations and inspect the results. Relations extraction uses lexico-syntactic patterns to identify IsA relations from unstructured text. Examples of lexico-syntactic patterns include:\n",
    "\n",
    " Lexico-syntactic pattern | Sample matching text\n",
    "  ------|------------------\n",
    "  <b>X</b> is a <b>Y</b>     | <i><b>apple</b> is a <b>fruit</b></i>, <i><b>switzerland</b> is a <b>country</b></i>\n",
    "  <b>X</b> such as <b>Y</b>     | <i><b>fruits</b> such as <b>mango</b></i>, <i><b>scientists</b> such as <b>Einstein</b></i>\n",
    "  <b>X</b> is an example of <b>Y</b>     | <i><b>iphone</b> is an example of <b>smartphone</b></i>\n",
    "  \n",
    "  <br>\n",
    "  In this exercise, we will use such lexico-syntactic patterns to identify IsA relations from text.\n",
    "  \n",
    "  \n",
    "  ###  Question 1.a\n",
    "\n",
    "  Load the given file <b>wiki_food_en.txt</b> into memory using the following code:\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(filename):\n",
    "    file_text = []\n",
    "    with open(filename, encoding=\"utf8\") as fp:\n",
    "        for line in fp:\n",
    "            file_text.append(line.strip().lower())\n",
    "    return \" \".join(file_text)\n",
    "\n",
    "file_text = load_text_file(\"wiki_food_en.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The following code uses the regular expression library to detect lexico-syntactic patterns in the file_text. The example below uses the regular expression \"X is a Y\". Fill in the blanks (...):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_matches(file_text, regexp_string):\n",
    "    #Compile a regular expression \n",
    "    regexp = re.compile(regexp_string)\n",
    "    \n",
    "    #Find all matches with the given regular expression\n",
    "    matches = re.findall(regexp, file_text)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "isa_matches = find_matches(file_text, \"[a-z]+ is a [a-z]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    " ###  Question 1.b\n",
    "\n",
    "Run the above code for relations extraction with the following lexico-syntactic patterns:\n",
    "\n",
    "<ol>\n",
    "  <li>X such as Y</li>\n",
    "  <li>such X as Y</li>\n",
    "<li>X and other Y</li>  \n",
    "</ol>\n",
    "\n",
    "Manually inspect the results and compute the accuracies of first 20 matches for each lexico-syntactic pattern. What do you observe? Is there any important difference between patterns no. 1,2 and 3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) pig such as the\n",
      "2) beverages such as cocktails\n",
      "3) emulsifiers such as egg\n",
      "4) varieties such as allioli\n",
      "5) pests such as the\n",
      "6) ceremonies such as weddings\n",
      "7) pests such as navel\n",
      "8) versions such as the\n",
      "9) pests such as the\n",
      "10) ceremonies such as weddings\n",
      "11) pests such as navel\n",
      "12) versions such as the\n",
      "13) illnesses such as infections\n",
      "14) pans such as a\n",
      "15) manufacturing such as acetic\n",
      "16) aid such as modified\n",
      "17) session such as of\n",
      "18) plants such as pear\n",
      "19) foods such as apple\n",
      "20) cheeses such as cheddar\n"
     ]
    }
   ],
   "source": [
    "matches = find_matches(file_text, \"[a-z]+ such as [a-z]+\")\n",
    "for i,m in enumerate(matches[0:20]):\n",
    "    print (str(i+1) + \") \" + m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Graph Construction\n",
    "\n",
    "As you noticed in the previous step, the output of lexico-syntactic patterns-based relations extraction contains significant noise. The task of noise removal is fairly involved and beyond the scope of this exercise. For further reading, we recommend this paper, which demonstrates a state-of-the-art effort for IsA relations extraction (<a href=\"http://webdatacommons.org/isadb/lrec2016.pdf\">A Large Database of Hypernymy Relations Extracted from the Web</a>).\n",
    "\n",
    "In this part of the exercise, we assume that IsA relations extracted using a state-of-the-art approach are already available. Given these relations, the aim of this step is to construct an initial potentially-noisy hypernym graph.\n",
    "\n",
    "###  Question 2.a\n",
    "\n",
    "\n",
    "Load the IsA relations of the food domain from the given file \"food_isa_relations.txt\" using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = []\n",
    "with open(\"food_isa_relations.txt\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        toks = line.strip().split('\\t')\n",
    "        rels.append((toks[0],toks[1],float(toks[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, graphs are better handled as a default 2-level dictionary. For example, the edge (<i>apple</i>,<i>fruit</i>, freq) is represented as a two-level map:\n",
    "\n",
    "map['apple']['fruit'] = freq\n",
    "\n",
    "The following code converts the IsA relations loaded from the file into a 2-level dictionary. Fill in the blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "noisy_relations = defaultdict(dict)\n",
    "for hypo, hyper, freq in rels:\n",
    "    noisy_relations[hypo][hyper] = freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "###  Question 2.b\n",
    "\n",
    "The next step of taxonomy induction involves removing and filtering out noisy IsA relations. In a real scenario, this usually involves a wide variety of steps. However, in this exercise, we will implement only one step. In this step, we will sort all the hypernyms for each hyponym, and only retain top-5 hypernyms for each hyponym.\n",
    "\n",
    "First, print the hypernyms of 'apple':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0.41334243636,\n",
       " 'brand': 0.0915626602388,\n",
       " 'company': 0.547120747653,\n",
       " 'company in the world': 0.0747098410221,\n",
       " 'crop': 0.0988772876235,\n",
       " 'flavour': 0.0858994706649,\n",
       " 'food': 0.147812587177,\n",
       " 'fruit': 1.0,\n",
       " 'fruit tree': 0.115782511138,\n",
       " 'hardware company': 0.0897298351702,\n",
       " 'ingredient': 0.068957229831,\n",
       " 'manufacturer': 0.079702775123,\n",
       " 'orange': 0.155466692675,\n",
       " 'product': 0.0697003665663,\n",
       " 'rival': 0.0788062030625,\n",
       " 'tag': 0.0736281177128,\n",
       " 'tech company': 0.120745147762,\n",
       " 'technology company': 0.0755438963724,\n",
       " 'tree': 0.110838111316,\n",
       " 'vegetable': 0.132148274312}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_relations['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Fill in the blanks in the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': 0.636619644821, 'asset': 0.458823529412, 'beverage': 0.690217289588, 'delicious goings-on around town': 1.0, 'drink': 0.56420713659}\n"
     ]
    }
   ],
   "source": [
    "# we dont need to order them in the noisy relations. just need to keep the top 5\n",
    "for hypo,_ in noisy_relations.items():\n",
    "    sorted_hypernyms = sorted(noisy_relations[hypo].items(), key = lambda x: -1*x[1])\n",
    "    noisy_relations[hypo] = {k:v for k,v in sorted_hypernyms[0:5]}\n",
    "    \n",
    "# Printed filtered noisy relations.\n",
    "print(noisy_relations['wine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont need to order them in the noisy relations. just need to keep the top 5\n",
    "for hypo in noisy_relations.keys():\n",
    "    sorted_hypernyms = sorted(noisy_relations[hypo].items(), key = lambda x: -1 *x[1])\n",
    "    noisy_relations[hypo] = {k:v for k,v in sorted_hypernyms[0:5]}\n",
    "    \n",
    "# Printed filtered noisy relations.\n",
    "print(noisy_relations['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.c\n",
    "\n",
    "In the next step, we would first convert the set of filtered IsA relations into a graph. First install the library networkx and matplotlib:\n",
    "\n",
    "   $ pip install networkx<br>\n",
    "   \n",
    "   $ pip install matplotlib\n",
    "   \n",
    "\n",
    "\n",
    "Further, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "G=nx.DiGraph()\n",
    "\n",
    "for hypo in noisy_relations.keys():\n",
    "    for hyper in noisy_relations[hypo].keys():\n",
    "        G.add_edge(hypo, hyper)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the paths between the following terms (Hint: use the networkx function all_simple_paths):\n",
    "\n",
    "1. 'apple' and 'food'\n",
    "2. 'fusilli' and 'food'\n",
    "3.  'okra' and 'food' \n",
    "\n",
    "Do you notice any relationship between the length of the path and its accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.algorithms.simple_paths.all_simple_paths.html\n",
    "# returns a generator (iterator?)\n",
    "\n",
    "print('In general, the shorter paths are more likely to be accurate. the longer paths are more likely to contain noisy edges.\\n')\n",
    "\n",
    "for p in nx.all_simple_paths(G, source='apple', target='food'):\n",
    "    print (p)\n",
    "print()\n",
    "for p in nx.all_simple_paths(G, source='banana', target='food'):\n",
    "    print (p)\n",
    "print()\n",
    "for p in nx.all_simple_paths(G, source='fusilli', target='food'):\n",
    "    print (p)\n",
    "print()\n",
    "for p in nx.all_simple_paths(G, source='okra', target='food'):\n",
    "    print (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.d\n",
    "\n",
    "In this step, we will now build a taxonomy. We will undertake the following steps:\n",
    "\n",
    "<ol>\n",
    "  <li> Let the vocabulary be {'apple', 'mango', 'peach', 'orange', 'banana'}.</li> \n",
    "<li> Let the root of the taxonomy be 'food'.</li> \n",
    "<li> Find all simple paths between terms in the vocabulary and the root. </li>\n",
    "<li> Retain all simple paths of length $l$.\n",
    "<li> Construct a graph by aggregatiing all the edges in the retained paths. </li>\n",
    "\n",
    "The below code implements the above steps. Fill in the blanks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paths(vocab, root, l):\n",
    "    retained_paths = []\n",
    "    for term in vocab:\n",
    "        for path in nx.all_simple_paths(G, source=term, target=root):\n",
    "            if len(path) < l:\n",
    "                retained_paths.append(path)\n",
    "    return retained_paths\n",
    "\n",
    "# keep only paths from select_paths\n",
    "def aggregate_paths(paths):\n",
    "    agg_graph = defaultdict(dict)\n",
    "    \n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        for i,term in enumerate(path[0:len(path) -1]):\n",
    "            agg_graph[term][path[i+1]] = 1\n",
    "            \n",
    "    return agg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = ['apple', 'mango', 'peach', 'orange', 'banana']\n",
    "root = 'food'\n",
    "\n",
    "graph = aggregate_paths(select_paths(V, root, 3))\n",
    "# Plot the graph\n",
    "Gt = nx.DiGraph()\n",
    "for k in graph.keys():\n",
    "    for k1 in graph[k].keys():\n",
    "        Gt.add_edge(k,k1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.e\n",
    "\n",
    "Plot the aggregated graph using the previous steps but with different path lengths (For example, 2 or 4). What do you notice? <font color='red'>lower precision but higher recall for longer lengths</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = aggregate_paths(select_paths(V, root, 4))\n",
    "# Plot the graph\n",
    "Gt = nx.DiGraph()\n",
    "for k in graph.keys():\n",
    "    for k1 in graph[k].keys():\n",
    "        Gt.add_edge(k,k1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2.f\n",
    "\n",
    "Repeat steps from 2.b to 2.e but without filtering the noisy relations in step 2.b. What do you observe? <font color='red'>lower precision but higher recall without filtering step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = aggregate_paths(select_paths(V, root, 4))\n",
    "# Plot the graph\n",
    "Gt = nx.DiGraph()\n",
    "for k in graph.keys():\n",
    "    for k1 in graph[k].keys():\n",
    "        Gt.add_edge(k,k1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DIS2018)",
   "language": "python",
   "name": "dis2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
