{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise we will understand the functioning of TF/IDF ranking. \n",
    "\n",
    "Implement the vector space retrieval model, based on the code framework provided below.\n",
    "\n",
    "For testing we have provided a simple document collection with 5 documents in file bread.txt:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q = ``baking''$, find the top ranked documents according to the TF/IDF rank.\n",
    "\n",
    "For further testing, use the collection __epfldocs.txt__, which contains recent tweets mentioning EPFL.\n",
    "\n",
    "Compare the results also to the results obtained from the reference implementation using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ha_ha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ha_ha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[['how', 'to', 'bake', 'bread', 'without', 'bake', 'recip'], ['smith', 'pie', 'best', 'pie', 'in', 'london'], ['numer', 'recip', 'the', 'art', 'of', 'scientif', 'comput'], ['bread', 'pastri', 'pie', 'and', 'cake', 'quantiti', 'bake', 'recip'], ['pastri', 'a', 'book', 'of', 'best', 'french', 'pastri', 'recip']]\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    \n",
    "    # remove special characters !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    \n",
    "    # break up a sentence into a list of words and punctuation with no whitespaces\n",
    "    # http://www.nltk.org/book/ch03.html\n",
    "    # input is one sentence\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # convert to lowercase words then have stemmers remove morphological affixes\n",
    "    # why join them into a string if we are going to split them at the end ?\n",
    "    # http://www.nltk.org/howto/stem.html\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])    \n",
    "    \n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "# strip to remove whitespace - https://docs.python.org/3/library/stdtypes.html#str.strip\n",
    "original_documents = [x.strip() for x in content] \n",
    "\n",
    "# after tokenize, split each document, resulting in a list of list\n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/9197844/maintain-count-in-python-list-comprehension\n",
    "# numbers = [[1,2,3,4,5,6],[2,3,4,5,6]]\n",
    "# [[x, sublist.count(x)] for sublist in numbers for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "#[item for sublist in documents for item in sublist]\n",
    "#[item for sublist in documents if 'how' in sublist]\n",
    "# https://stackoverflow.com/questions/17657720/python-list-comprehension-double-for\n",
    "\n",
    "# collection = [['a','b','b','c','d','e','f'],['b','b','d','f']]\n",
    "# [item for sublist in collection for item in sublist]\n",
    "# [item for sublist in collection if 'a' in sublist]\n",
    "# [item for sublist in collection for item in sublist if 'b' == item]\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.8325814637483102, 0.0, 0.0, 0.9162907318741551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22314355131420976, 0.0, 0.0, 1.6094379124341003], [0.0, 0.0, 0.9162907318741551, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6094379124341003, 0.0, 0.0, 1.8325814637483102, 0.0, 0.0, 0.0, 1.6094379124341003, 0.0], [1.6094379124341003, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6094379124341003, 0.0, 0.0, 1.6094379124341003, 0.0, 0.0, 0.0, 0.22314355131420976, 1.6094379124341003, 0.0, 0.0], [0.0, 0.9162907318741551, 0.0, 0.0, 0.9162907318741551, 1.6094379124341003, 0.0, 0.0, 0.0, 0.0, 0.9162907318741551, 0.9162907318741551, 1.6094379124341003, 0.22314355131420976, 0.0, 0.0, 0.0], [0.0, 0.0, 0.9162907318741551, 1.6094379124341003, 0.0, 0.0, 0.0, 1.6094379124341003, 0.0, 0.0, 1.8325814637483102, 0.0, 0.0, 0.22314355131420976, 0.0, 0.0, 0.0]]\n",
      "How to Bake Breads Without Baking Recipes 0.7010969225741529\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes 0.32179527698963223\n",
      "Pastry: A Book of Best French Pastry Recipes 0.0171952097639166\n",
      "Numerical Recipes: The Art of Scientific Computing 0.01636361668514889\n",
      "Smith Pies: Best Pies in London 0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TF/IDF code\n",
    "\n",
    "# create the vocabulary, an unordered collection of unique elements\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "\n",
    "# remove all stopwords \n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "\n",
    "# sort to alphabetical order\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {} # initialize dictionary\n",
    "    num_documents = len(documents)    \n",
    "    for i, term in enumerate(vocabulary):                \n",
    "        # compute the number of documents that contain each term\n",
    "        idf[term] = math.log(num_documents/sum([term in x for x in documents]), math.e)    \n",
    "                \n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "# compute tf-idf\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        vector[i] = document.count(term)/max_count * idf[term]\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i];\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy / (math.sqrt(sumxx)*math.sqrt(sumyy))\n",
    "    return result\n",
    "\n",
    "# computing the search result (get the topk documents)\n",
    "def search_vec(query, topk):\n",
    "    \n",
    "    # process text\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    \n",
    "    # compute cosine similarity  \n",
    "    # scores is a list the score for each document formatted as [score, document_index]\n",
    "    # scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores = []\n",
    "    for i,d in enumerate(document_vectors):\n",
    "        scores.append([cosine_similarity(query_vector, d), i])\n",
    "    \n",
    "    # sort scores by its first element\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(topk):\n",
    "            print(original_documents[scores[i][1]], scores[i][0])\n",
    "\n",
    "# HINTS\n",
    "# natural logarithm function\n",
    "#     math.log(n,math.e)\n",
    "# Function to count term frequencies in a document\n",
    "#     Counter(document)\n",
    "# most common elements for a list\n",
    "#     counts.most_common(1)\n",
    "print(document_vectors)\n",
    "print(search_vec('baking recipe',5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Recipes: The Art of Scientific Computing 0.481270078046231\n",
      "Pastry: A Book of Best French Pastry Recipes 0.0\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes 0.0\n",
      "Smith Pies: Best Pies in London 0.0\n",
      "How to Bake Breads Without Baking Recipes 0.0\n"
     ]
    }
   ],
   "source": [
    "# Reference code using scikit-learn\n",
    "\n",
    "# Initialize settings\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "\n",
    "# Compute tf-idf \n",
    "features = tf.fit_transform(original_documents)\n",
    "\n",
    "# Convert features to \"matrix form\"\n",
    "npm_tfidf = features.todense()\n",
    "#print(tf.get_feature_names()) #print feature names\n",
    "#print(features)               #print tf-idf scores\n",
    "\n",
    "# Transform query\n",
    "new_features = tf.transform(['numerical'])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "\n",
    "# Sort and retain indices\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "# Print top scores\n",
    "topk = 5\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]], cosine_similarities[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2\n",
    "\n",
    "Implement probabilistic retrieval model based on the query likelihood language model, using a mixture model between the documents and the collection, both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. You can use the code framework provided below.\n",
    "\n",
    "Now, for the query $Q = ``baking''$, find the top ranked documents according to the probabilistic rank.\n",
    "\n",
    "Compare the results with TF/IDF ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes 0.00045208163265306124\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes 0.000251125\n",
      "Numerical Recipes: The Art of Scientific Computing 0.00021942857142857143\n",
      "Smith Pies: Best Pies in London 4.8e-05\n",
      "Pastry: A Book of Best French Pastry Recipes 4.8e-05\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Probabilistic retrieval code\n",
    "\n",
    "# smoothing parameter\n",
    "lmbda = 0.5\n",
    "\n",
    "# term frequency\n",
    "def tf(word, document):\n",
    "    return document.count(word)/len(document)\n",
    "\n",
    "# collection size\n",
    "# total number of terms in collection\n",
    "def collection_size(documents):\n",
    "    cs = 0\n",
    "    for d in documents:\n",
    "        cs += len(documents)\n",
    "    return cs\n",
    "\n",
    "# collection frequency\n",
    "def cf(word, documents):\n",
    "    cf = 0\n",
    "    for d in documents:\n",
    "        cf += d.count(word)\n",
    "    return cf/collection_size(documents)\n",
    "\n",
    "# probabilistic relevance\n",
    "# probability that the document generated the query\n",
    "def query_prob(query, document):\n",
    "    prob = 1\n",
    "    \n",
    "    for q in query:\n",
    "        prob *= (lmbda * tf(q, document) + (1-lmbda) * cf(q, documents))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "# computing the search result\n",
    "def search_prob(query, k):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    scores = [[query_prob(q, documents[d]),d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(len(k)):\n",
    "            print(original_documents[scores[i][1]], scores[i][0])\n",
    "\n",
    "# HINTS\n",
    "# counting occurrences of a word in a document:\n",
    "#     document.count(word)\n",
    "# length of a document:\n",
    "#     len(document)\n",
    "# querying:\n",
    "#     print(search_prob(\"How\", documents))\n",
    "print(search_prob(\"Numerical Breads Baking\", documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Following the notation used in class, let us denote the set of terms by $T=\\{k_i|i=1,...,m\\}$, the set of documents by $D=\\{d_j |j=1,...,n\\}$, and let $d_i=(w_{1j},w_{2j},...,w_{mj})$. We are also given a query  $q=(w_{1q},w_{2q},...,w_{mq})$. In the lecture we studied that, \n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} \\frac{w_{ij}}{|d_j|}\\frac{w_{iq}}{|q|}$ .  (1)\n",
    "\n",
    "Another way of looking at the information retrieval problem is using a probabilistic approach. The probabilistic view of information retrieval consists of determining the conditional probability $P(q|d_j)$ that for a given document $d_j$ the query by the user is $q$. So, practically in probabilistic retrieval when a query $q$ is given, for each document it is evaluated how probable it is that the query is indeed relevant for the document, which results in a ranking of the documents.\n",
    "\n",
    "In order to relate vector space retrieval to a probabilistic view of information retrieval, we interpret the weights in Equation (1) as follows:\n",
    "\n",
    "-  $w_{ij}/|d_j|$ can be interpreted as the conditional probability $P(k_i|d_j)$ that for a given document $d_j$ the term $k_i$ is important (to characterize the document $d_j$).\n",
    "\n",
    "-  $w_{iq}/|q|$ can be interpreted as the conditional probability $P(q|k_i)$ that for a given term $k_i$ the query posed by the user is $q$. Intuitively, $P(q|k_i)$ gives the amount of importance given to a particular term while querying. <font color=\"red\">Why not $P(k_i|q)$</font>\n",
    "\n",
    "With this interpretation you can rewrite Equation (1) as follows:\n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} P(k_i|d_j)P(q|k_i)$ (2)\n",
    "\n",
    "### Question a\n",
    "Show that indeed with the probabilistic interpretation of weights of vector space retrieval, as given in Equation (2), the similarity computation in vector space retrieval results exactly in the probabilistic interpretation of information retrieval, i.e., $sim(q,d_j)= P(q|d_j)$.\n",
    "Given that $d_j$ and $q$ are conditionally independent, i.e., $P(d_j \\cap q|ki) = P(d_j|k_i)P(q|k_i)$. You can assume existence of joint probability density functions wherever required. (Hint: You might need to use Bayes theorem)\n",
    "\n",
    "### Solution\n",
    "\n",
    "\\begin{align}\n",
    "P(q|d_j) &= \\dfrac{P(d_j|q)P(q)}{P(d_j)} \\\\\n",
    "\\sum^m_{i=1} P(q,k_i|d_j) &= \\dfrac{\\sum^m_{i=1} P(q,k_i,d_j)}{P(d_j)} \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j,q|k_i)P(k_i)}{P(d_j)} \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j|k_i)P(q|k_i)P(k_i)}{P(d_j)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "sim(q,d_j) &= \\sum^m_{i=1} P(k_i|d_j)P(q|k_i) \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j|k_i)P(k_i)P(q|k_i) }{P(d_j)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b\n",
    "Using the expression derived for $P(q|d_j)$ in (a), obtain a ranking (documents sorted in descending order of their scores) for the documents $P(k_i|d_1) = (0, 1/3, 2/3)$, $P(k_i|d_2) =(1/3, 2/3, 0)$, $P(k_i|d_3) = (1/2, 0, 1/2)$, and $P (k_i|d_4) = (3/4, 1/4, 0)$ and the query $P(q|k_i) = (1/5, 0, 2/3)$.\n",
    "\n",
    "\\begin{align}\n",
    "sim(q,d_1) &= \\sum^m_{i=1} P(k_i|d_j)P(q|k_i) \\\\\n",
    "&= 0*\\dfrac{1}{5} + \\dfrac{1}{3}*0 + \\dfrac{2}{3}*\\dfrac{2}{3} \\\\\n",
    "&= \\dfrac{4}{9}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "sim(q,d_2) &= \\sum^m_{i=1} P(k_i|d_j)P(q|k_i) \\\\\n",
    "&= \\dfrac{1}{3}*\\dfrac{1}{5} + \\dfrac{2}{3}*0 + 0*\\dfrac{2}{3} \\\\\n",
    "&= \\dfrac{1}{15}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "sim(q,d_3) &= \\sum^m_{i=1} P(k_i|d_j)P(q|k_i) \\\\\n",
    "&= \\dfrac{1}{2}*\\dfrac{1}{5} + 0*0 + \\dfrac{1}{2}*\\dfrac{2}{3} \\\\\n",
    "&= \\dfrac{13}{30}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DIS2018)",
   "language": "python",
   "name": "dis2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
