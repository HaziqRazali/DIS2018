{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise we will understand the functioning of TF/IDF ranking. \n",
    "\n",
    "Implement the vector space retrieval model, based on the code framework provided below.\n",
    "\n",
    "For testing we have provided a simple document collection with 5 documents in file bread.txt:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q = ``baking''$, find the top ranked documents according to the TF/IDF rank.\n",
    "\n",
    "For further testing, use the collection __epfldocs.txt__, which contains recent tweets mentioning EPFL.\n",
    "\n",
    "Compare the results also to the results obtained from the reference implementation using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ha_ha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ha_ha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # remove special characters !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    \n",
    "    # break up a sentence into a list of words and punctuation with no whitespaces\n",
    "    # http://www.nltk.org/book/ch03.html\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # convert to lowercase words then have stemmers remove morphological affixes\n",
    "    # why join them into a string if we are going to split them at the end ?\n",
    "    # http://www.nltk.org/howto/stem.html\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])    \n",
    "    \n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "# strip to remove whitespace - https://docs.python.org/3/library/stdtypes.html#str.strip\n",
    "original_documents = [x.strip() for x in content] \n",
    "\n",
    "# after tokenize, split each document, resulting in a list of list\n",
    "documents = [tokenize(d).split() for d in original_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/9197844/maintain-count-in-python-list-comprehension\n",
    "numbers = [[1,2,3,4,5,6],[2,3,4,5,6]]\n",
    "[[x, sublist.count(x)] for sublist in numbers for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[item for sublist in documents for item in sublist]\n",
    "#[item for sublist in documents if 'how' in sublist]\n",
    "# https://stackoverflow.com/questions/17657720/python-list-comprehension-double-for\n",
    "\n",
    "collection = [['a','b','b','c','d','e','f'],['b','b','d','f']]\n",
    "[item for sublist in collection for item in sublist]\n",
    "#[item for sublist in collection if 'a' in sublist]\n",
    "[item for sublist in collection for item in sublist if 'b' == item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes 0.7008061185835583\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes 0.31265750124694125\n",
      "Smith Pies: Best Pies in London 0\n",
      "Numerical Recipes: The Art of Scientific Computing 0\n",
      "Pastry: A Book of Best French Pastry Recipes 0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TF/IDF code\n",
    "\n",
    "# create the vocabulary\n",
    "# create an unordered collection of unique elements\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "\n",
    "# remove all stopwords \n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "\n",
    "# sort to alphabetical order\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)    \n",
    "    for i, term in enumerate(vocabulary):                \n",
    "        # compute the number of documents each term appears in the collection \n",
    "        idf[term] = math.log(num_documents/sum(term in x for x in documents), math.e)         \n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "# compute tf-idf\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        vector[i] = document.count(term)/max_count * idf[term]\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i];\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy / (math.sqrt(sumxx)*math.sqrt(sumyy))\n",
    "    return result\n",
    "\n",
    "# computing the search result (get the topk documents)\n",
    "def search_vec(query, topk):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(topk):\n",
    "            print(original_documents[scores[i][1]], scores[i][0])\n",
    "\n",
    "# HINTS\n",
    "# natural logarithm function\n",
    "#     math.log(n,math.e)\n",
    "# Function to count term frequencies in a document\n",
    "#     Counter(document)\n",
    "# most common elements for a list\n",
    "#     counts.most_common(1)\n",
    "print(search_vec('baking',5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code using scikit-learn\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "new_features = tf.transform(['baking'])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "topk = 5\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]])\n",
    "    \n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2\n",
    "\n",
    "Implement probabilistic retrieval model based on the query likelihood language model, using a mixture model between the documents and the collection, both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. You can use the code framework provided below.\n",
    "\n",
    "Now, for the query $Q = ``baking''$, find the top ranked documents according to the probabilistic rank.\n",
    "\n",
    "Compare the results with TF/IDF ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)\n",
    "print([subdoc.count(term) for subdoc in documents])\n",
    "print(sum([subdoc.count(term) for subdoc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Probabilistic retrieval code\n",
    "\n",
    "# smoothing parameter\n",
    "lmbda = 0.5\n",
    "\n",
    "# term frequency\n",
    "def tf(word, document):    \n",
    "    # https://stackoverflow.com/questions/2600191/how-to-count-the-occurrences-of-a-list-item\n",
    "    # document.count(word) also works. was not working earlier due to a bug probably in query_prob\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1] \n",
    "    return counts[word]/float(max_count)\n",
    "\n",
    "# collection frequency\n",
    "def cf(word, documents):  \n",
    "    # number of times term appears in collection / total number of terms in collection\n",
    "    return sum(subdoc.count(word) for subdoc in documents)/collection_term_size(documents)\n",
    "\n",
    "# collection term size\n",
    "def collection_term_size(documents):\n",
    "    return sum([len(sublist) for sublist in documents])    \n",
    "\n",
    "# probabilistic relevance\n",
    "def query_prob(query, document, documents):      \n",
    "    prob = 1\n",
    "    for q in query:\n",
    "        prob *= (lmbda*tf(q, document)/len(document) + (1-lmbda)*cf(q, documents)) \n",
    "        #print(lmbda, ' * ', tf(q, document)/len(document), ' + ',  (1-lmbda), '*', cf(q, documents))\n",
    "    return prob\n",
    "\n",
    "# computing the search result\n",
    "def search_prob(query, k):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    scores = [[query_prob(q, documents[d], documents),d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(len(k)):\n",
    "            print(scores[i][0])\n",
    "            print(original_documents[scores[i][1]])\n",
    "\n",
    "# HINTS\n",
    "# counting occurrences of a word in a document:\n",
    "#     document.count(word)\n",
    "# length of a document:\n",
    "#     len(document)\n",
    "# querying:\n",
    "#     print(search_prob(\"How\", documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5  *  0.0  +  0.5 * 0.07692307692307693\n",
      "0.5  *  0.14285714285714285  +  0.5 * 0.15384615384615385\n",
      "0.5  *  0.16666666666666666  +  0.5 * 0.07692307692307693\n",
      "0.5  *  0.16666666666666666  +  0.5 * 0.15384615384615385\n",
      "0.019518408941485862\n",
      "Albert Einstein received the nobel prize\n",
      "0.005705832628909552\n",
      "Einstein was one of the greatest scientists\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(search_prob(\"Albert Einstein\", documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Following the notation used in class, let us denote the set of terms by $T=\\{k_i|i=1,...,m\\}$, the set of documents by $D=\\{d_j |j=1,...,n\\}$, and let $d_i=(w_{1j},w_{2j},...,w_{mj})$. We are also given a query  $q=(w_{1q},w_{2q},...,w_{mq})$. In the lecture we studied that, \n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} \\frac{w_{ij}}{|d_j|}\\frac{w_{iq}}{|q|}$ .  (1)\n",
    "\n",
    "Another way of looking at the information retrieval problem is using a probabilistic approach. The probabilistic view of information retrieval consists of determining the conditional probability $P(q|d_j)$ that for a given document $d_j$ the query by the user is $q$. So, practically in probabilistic retrieval when a query $q$ is given, for each document it is evaluated how probable it is that the query is indeed relevant for the document, which results in a ranking of the documents.\n",
    "\n",
    "In order to relate vector space retrieval to a probabilistic view of information retrieval, we interpret the weights in Equation (1) as follows:\n",
    "\n",
    "-  $w_{ij}/|d_j|$ can be interpreted as the conditional probability $P(k_i|d_j)$ that for a given document $d_j$ the term $k_i$ is important (to characterize the document $d_j$).\n",
    "\n",
    "-  $w_{iq}/|q|$ can be interpreted as the conditional probability $P(q|k_i)$ that for a given term $k_i$ the query posed by the user is $q$. Intuitively, $P(q|k_i)$ gives the amount of importance given to a particular term while querying.\n",
    "\n",
    "With this interpretation you can rewrite Equation (1) as follows:\n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} P(k_i|d_j)P(q|k_i)$ (2)\n",
    "\n",
    "### Question a\n",
    "Show that indeed with the probabilistic interpretation of weights of vector space retrieval, as given in Equation (2), the similarity computation in vector space retrieval results exactly in the probabilistic interpretation of information retrieval, i.e., $sim(q,d_j)= P(q|d_j)$.\n",
    "Given that $d_j$ and $q$ are conditionally independent, i.e., $P(d_j \\cap q|ki) = P(d_j|k_i)P(q|k_i)$. You can assume existence of joint probability density functions wherever required. (Hint: You might need to use Bayes theorem)\n",
    "\n",
    "### Solution\n",
    "\n",
    "\\begin{align}\n",
    "P(q|d_j) &= \\dfrac{P(d_j|q)P(q)}{P(d_j)} \\\\\n",
    "\\sum^m_{i=1} P(q,k_i|d_j) &= \\dfrac{\\sum^m_{i=1} P(q,k_i,d_j)}{P(d_j)} \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j,q|k_i)P(k_i)}{P(d_j)} \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j|k_i)P(q|k_i)P(k_i)}{P(d_j)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "sim(q,d_j) &= \\sum^m_{i=1} P(k_i|d_j)P(q|k_i) \\\\\n",
    "&= \\dfrac{\\sum^m_{i=1} P(d_j|k_i)P(k_i)P(q|k_i) }{P(d_j)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question b\n",
    "Using the expression derived for $P(q|d_j)$ in (a), obtain a ranking (documents sorted in descending order of their scores) for the documents $P(k_i|d_1) = (0, 1/3, 2/3)$, $P(k_i|d_2) =(1/3, 2/3, 0)$, $P(k_i|d_3) = (1/2, 0, 1/2)$, and $P (k_i|d_4) = (3/4, 1/4, 0)$ and the query $P(q|k_i) = (1/5, 0, 2/3)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DIS2018)",
   "language": "python",
   "name": "dis2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
