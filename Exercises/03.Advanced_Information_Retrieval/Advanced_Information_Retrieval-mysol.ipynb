{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Advanced Information Retrieval\n",
    "\n",
    "## Question 1 - Latent Semantic Indexing\n",
    "\n",
    "\n",
    "In this exercise, we will run latent semantic indexing on a term-document matrix using python numpy library.\n",
    "\n",
    "Suppose we are given the following term-document matrix containing eleven terms and four documents $d_1$ , $d_2$ , $d_3$ and $d_4$:\n",
    "\n",
    "$\n",
    "M =\n",
    "  \\begin{bmatrix}\n",
    "    d_1 & d_2 & d_3 & d_4 \\\\ \n",
    "\t1 & 1 & 1 & 1  \\\\\n",
    "\t0 & 1 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 0 \\\\\n",
    "\t0 & 1 & 0 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    1 & 0 & 1 & 2 \\\\\n",
    "    1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 1 & 0 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 2 & 1 & 1 \\\\\n",
    "    0 & 1 & 1 & 0 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "###  Question 1.a\n",
    "\n",
    "Compute the singular value decomposition of the term-document matrix M. Print the values of the output matrices $K$, $S$ and $D^t$.\n",
    "\n",
    "\n",
    "<b>Hint:</b> Use the function numpy.linalg.svd. More details of this function can be found here at this link:\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.9709505944546686\n",
      "0.4\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def H(a,b):\n",
    "    return -(a/(a+b))*np.log2(a/(a+b)) -(b/(a+b))*np.log2(b/(a+b))\n",
    "\n",
    "print(-(2/5)*np.log2(2/5) -(3/5)*np.log2(3/5))\n",
    "print(H(2,3))\n",
    "print(H(1,1)*2/5)\n",
    "print(H(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4) (4,) (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# import Python matrix operations library\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "#set M matrix using the given values.\n",
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "\n",
    "M = np.array(M)\n",
    "\n",
    "# compute SVD\n",
    "# False returns truncated matrices\n",
    "# True returns square matrices\n",
    "K,S,Dt = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "print(np.shape(K), np.shape(S), np.shape(Dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36838448 -0.57010731 -0.53356439 -0.50455879]\n",
      " [-0.74000417  0.61762211  0.0885323  -0.25119473]\n",
      " [ 0.54948837  0.36008671 -0.05294924 -0.75206148]\n",
      " [-0.12144645 -0.40479395  0.83944473 -0.34165065]]\n"
     ]
    }
   ],
   "source": [
    "print(Dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Question 1.b\n",
    "\n",
    "Are the values of $S$ sorted? Perform latent semantic indexing by selecting the first two largest singular values of the matrix $S$.\n",
    "\n",
    "<b>Hint:</b> See the lecture slides on latent semantic indexing for more details. A sub-matrix of a numpy matrix can be computed using indexing operations (see https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36838448 -0.74000417]\n",
      " [-0.57010731  0.61762211]\n",
      " [-0.53356439  0.0885323 ]\n",
      " [-0.50455879 -0.25119473]]\n"
     ]
    }
   ],
   "source": [
    "K_sel = K[:,0:2]\n",
    "S_sel = S[0:2]\n",
    "Dt_sel = Dt[0:2,:]\n",
    "\n",
    "# don't need to compute D_sel\n",
    "D_sel = np.transpose(Dt_sel)\n",
    "print(D_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.c\n",
    "\n",
    "Given the query $q$:\n",
    "\n",
    "$\n",
    "q =\n",
    "  \\begin{bmatrix}\n",
    "\t0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "Map query $q$ into the new document space $D$. The new query is referred to as $q^*$. \n",
    "\n",
    "<b>Hint:</b> Use the formulation for mapping queries provided in the lecture slides. You can also use np.linalg.inv function for computing the inverse of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1, Explicit transformation of q*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22662409  0.11624731]\n"
     ]
    }
   ],
   "source": [
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "qstar = np.transpose(q).dot(K_sel).dot(np.linalg.inv(np.diag(S_sel)))\n",
    "qstar = np.squeeze(qstar)\n",
    "print(qstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2, SVD of M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36838448 -0.57010731 -0.53356439 -0.50455879]\n",
      " [-0.74000417  0.61762211  0.0885323  -0.25119473]\n",
      " [ 0.54948837  0.36008671 -0.05294924 -0.75206148]\n",
      " [-0.12144645 -0.40479395  0.83944473 -0.34165065]]\n",
      "(11, 5) (5,) (5, 5)\n",
      "[[-0.34678427 -0.55465547 -0.52063076 -0.49410246 -0.23854631]\n",
      " [-0.77634761  0.55827622  0.06842344 -0.23283123  0.16346226]\n",
      " [ 0.45759425  0.46584155 -0.00393171 -0.66474561 -0.36290077]\n",
      " [-0.21191057  0.1386913  -0.01252367  0.42561094 -0.86865247]\n",
      " [-0.15075303 -0.3800544   0.85093451 -0.28039432 -0.17355593]]\n"
     ]
    }
   ],
   "source": [
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "M = np.concatenate((M,q[:,np.newaxis]), axis=1)\n",
    "\n",
    "print(Dt)\n",
    "K,S,Dt = np.linalg.svd(M, full_matrices=False)\n",
    "print(np.shape(K), np.shape(S), np.shape(Dt))\n",
    "print(Dt)\n",
    "\n",
    "#Dt_sel = Dt[0:2,:]\n",
    "#print(np.shape(Dt_sel))\n",
    "#print(Dt_sel[:,-1])\n",
    "#print(Dt_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.d\n",
    "\n",
    "Arrange the documents based on the cosine similarity measure between $q^*$ and the new documents in the space $D$.\n",
    "\n",
    "<b>Hint:</b> Use the cosine similarity function from the previous exercise on vector space retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(np.shape(D_sel)[0]):\n",
    "    scores.append([cosine_similarity(qstar,D_sel[i,:]),i])    \n",
    "scores.sort(key=lambda x: -x[0])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1.e\n",
    "\n",
    "Does the order of documents change if document $d_3$ is dropped? If yes, why? \n",
    "If no, how should $d_3$ be modified to change the document ordering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.f [Optional]\n",
    "\n",
    "Run latent semantic indexing for the document collection presented in the previous exercise (presented here as well):\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "Now, for the query $Q=$''<i>baking</i>'', find the top ranked documents according to LSI (use three singular values). \n",
    "\n",
    "<b>Hint:</b> Use the code for computing document_vectors from the last exercise. However note that document_vectors represent document-term matrix whereas LSI uses term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "# strip to remove whitespace - https://docs.python.org/3/library/stdtypes.html#str.strip\n",
    "original_documents = [x.strip() for x in content] \n",
    "\n",
    "# after tokenize, split each document, resulting in a list of list\n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "document_vectors = np.transpose(document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the SVD and extract the top 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute SVD\n",
    "K,S,Dt = np.linalg.svd(document_vectors)\n",
    "\n",
    "# select top 2\n",
    "K_sel = K[:,0:2]\n",
    "S_sel = S[0:2]\n",
    "Dt_sel = Dt[0:2,:]\n",
    "document_vectors_sel = np.transpose(document_vectors).dot(K_sel).dot(np.linalg.inv(np.diag(S_sel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"baking\"\n",
    "q = query.split()\n",
    "q = [stemmer.stem(w) for w in q]\n",
    "query_vector = vectorize(q, vocabulary, idf)\n",
    "\n",
    "query_vector_star = np.transpose(query_vector).dot(K_sel).dot(np.linalg.inv(np.diag(S_sel)))\n",
    "query_vector_star = np.squeeze(query_vector_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(np.shape(document_vectors_sel)[0]):\n",
    "    scores.append([cosine_similarity(query_vector_star,document_vectors_sel[i,:]),i])    \n",
    "scores.sort(key=lambda x: -x[0])\n",
    "\n",
    "# show result\n",
    "for i in range(len(document_vectors_sel)):\n",
    "    print(original_documents[scores[i][1]], scores[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2 - Word Embeddings\n",
    "\n",
    "In this exercise, we would train word embeddings using a state-of-the-art embeddings library fastText. The first step of the exercise is to install the fastText library. Proceed with the following steps:\n",
    "\n",
    "### FastText installation\n",
    "\n",
    "\n",
    "#### Run these commands on the shell terminal:\n",
    "\n",
    "> wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip <br>\n",
    "> unzip v0.1.0.zip<br>\n",
    "> cd fastText-0.1.0 <br>\n",
    "> make<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Move the epfldocs.txt file (provided in the last exercise) to the current directory. Sample command (linux) for copying the file into current directory is as follows:\n",
    "\n",
    "> cp directory_path/epfldocs.txt ./\n",
    "\n",
    "<br>\n",
    "\n",
    "### Generate Embeddings\n",
    "\n",
    "Further, generate fasttext embeddings for the epfldocs.txt file using the following command:\n",
    "\n",
    "> ./fasttext skipgram -input epfldocs.txt -output model_epfldocs\n",
    "\n",
    "\n",
    "The above command generates word embeddings and stores them in a file named model_epfldocs.vec.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Load Embeddings\n",
    "\n",
    "In the second phase of this exercise, we will load these embeddings into memory using python and visualize them.\n",
    "Use the following python code to load the embeddings into memory:<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        lines = lines[1:]\n",
    "        vocabulary, wv = zip(*[line.strip().split(' ', 1) for line in lines])\n",
    "    wv = np.loadtxt(wv)\n",
    "    return wv, vocabulary\n",
    "\n",
    "\n",
    "# Replace the path based on your own machine.\n",
    "word_embeddings, vocabulary = load_embeddings(os.getcwd() + '/fastText-0.1.0/model_epfldocs.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings. First install the tsne library using pip: https://github.com/danielfrg/tsne/issues/4\n",
    "\n",
    "> $ sudo apt-get install libblas-dev libatlas-base-dev\n",
    "\n",
    "> $ pip install tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne import bh_sne\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "vis_data = bh_sne(word_embeddings)\n",
    "\n",
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a\n",
    "\n",
    "Observe the plot of word embeddings. Do you observe any patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.b\n",
    "\n",
    "Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings.\n",
    "\n",
    "Find the top 3 terms that are most similar to 'la', 'EPFL', '#robot', 'this',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we1 = word_embeddings[vocabulary.index('EPFL')]\n",
    "scores = []\n",
    "for i in range(len(vocabulary)):\n",
    "    scores.append([cosine_similarity(we1,word_embeddings[i,:]),i])\n",
    "scores.sort(key=lambda x: -x[0])\n",
    "\n",
    "# show result\n",
    "for i in range(3):\n",
    "    print(vocabulary[scores[i][1]], scores[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c [Optional]\n",
    "\n",
    "Download the text file using the following command:\n",
    "\n",
    "> wget http://mattmahoney.net/dc/text8.zip -O text8.gz <br>\n",
    "> tar -xvf text8.gz \n",
    "\n",
    "\n",
    "The above command creates a text file named 'text8'. Regenerate the fasttext embeddings using the text8 file. Plot the word embeddings for first 1000 terms in the vocabulary.\n",
    "\n",
    "### Question 2.d [Optional]\n",
    "\n",
    "Observe the word embeddings that are visualized in this link http://www.anthonygarvan.com/wordgalaxy/ . Can you make some interesting observations? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DIS2018)",
   "language": "python",
   "name": "dis2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
